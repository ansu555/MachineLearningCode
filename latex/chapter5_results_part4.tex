\subsection{Hybrid Model: SARIMA + XGBoost}

The hybrid model represents the pinnacle of our modeling approach, achieving the best overall performance by combining the complementary strengths of statistical and machine learning methods.

\subsubsection{Methodology and Architecture}

\textbf{Conceptual Framework:}

Financial time series can be decomposed into components:
\begin{equation}
y_t = L_t + N_t + \epsilon_t
\end{equation}

where:
\begin{itemize}
    \item $L_t$: Linear component (trend, seasonality, autocorrelation)
    \item $N_t$: Non-linear component (regime shifts, complex interactions)
    \item $\epsilon_t$: Random noise (irreducible error)
\end{itemize}

The hybrid approach explicitly models this decomposition:
\begin{itemize}
    \item \textbf{SARIMA} captures $L_t$ through its ARMA structure and differencing
    \item \textbf{XGBoost} learns $N_t$ from the residuals $r_t = y_t - \hat{L}_t$
\end{itemize}

\textbf{Three-Stage Pipeline:}

\textbf{Stage 1: SARIMA Baseline Training}
\begin{enumerate}
    \item Train SARIMA$(0,1,0) \times (0,0,0)_{[5]}$ on training data
    \item Generate in-sample predictions: $\hat{y}_t^{SARIMA}$
    \item Compute residuals: $r_t = y_t - \hat{y}_t^{SARIMA}$
\end{enumerate}

\textbf{Stage 2: XGBoost Residual Modeling}
\begin{enumerate}
    \item Engineer features from original data:
    \begin{itemize}
        \item Lag features (1, 2, 3, 5, 10 days)
        \item Technical indicators (MACD, RSI, Bollinger Bands)
        \item Temporal features (day of week, month, quarter)
    \end{itemize}
    \item Train XGBoost to predict residuals: $r_t \sim f(\mathbf{X}_t)$
    \item Key insight: Residuals are approximately stationary (trend removed by SARIMA)
    \item This allows XGBoost to work in a favorable regime without extrapolation issues
\end{enumerate}

\textbf{Stage 3: Hybrid Fusion}
\begin{equation}
\hat{y}_t^{Hybrid} = \hat{y}_t^{SARIMA} + \hat{r}_t^{XGBoost}
\end{equation}

At prediction time:
\begin{enumerate}
    \item SARIMA forecasts next-day price
    \item XGBoost predicts expected residual
    \item Final prediction combines both
\end{enumerate}

\subsubsection{Implementation Details}

\textbf{SARIMA Component Configuration:}
\begin{itemize}
    \item Model: SARIMA$(0,1,0) \times (0,0,0)_{[5]}$
    \item Differencing order: 1
    \item Seasonal period: 5 (weekly pattern in trading days)
    \item Fitting method: Maximum likelihood estimation
    \item Standalone RMSE: 3.24
\end{itemize}

\textbf{XGBoost Component Configuration:}
\begin{itemize}
    \item Objective: reg:squarederror
    \item Number of estimators: 100 trees
    \item Learning rate: 0.1
    \item Max depth: 5
    \item Subsample: 0.8 (row sampling for robustness)
    \item Colsample by tree: 0.8 (column sampling for diversity)
    \item Early stopping: patience=10 on validation RMSE
\end{itemize}

\textbf{Feature Set for XGBoost:}

Based on the actual notebook implementation:
\begin{itemize}
    \item \textbf{Lag features}: Close_{t-1}, Close_{t-2}, Close_{t-3}, Close_{t-5}, Close_{t-10}
    \item \textbf{MACD}: Moving Average Convergence Divergence and signal line
    \item \textbf{RSI}: 14-day Relative Strength Index
    \item \textbf{Bollinger Bands}: Upper, middle, lower bands (20-day window)
    \item \textbf{SMA}: Simple Moving Averages (5, 10, 20, 50 days)
    \item \textbf{Volume features}: Volume moving averages and changes
    \item \textbf{Temporal}: Day of week, month, quarter indicators
\end{itemize}

\subsubsection{Performance Results}

\textbf{Overall Metrics:}
\begin{itemize}
    \item \textbf{RMSE:} 1.28 (best among all models)
    \item \textbf{$R^2$ Score:} 0.9983 (explains 99.83\% of variance)
    \item \textbf{MAE:} 0.95
    \item \textbf{Directional Accuracy:} ~87\% (correctly predicts up/down movement)
\end{itemize}

\textbf{Improvement Over Components:}
\begin{itemize}
    \item 60.5\% better than SARIMA alone (RMSE: 3.24 → 1.28)
    \item 95.3\% better than XGBoost alone (RMSE: 27.13 → 1.28)
    \item 56.2\% better than Linear Regression (RMSE: 2.92 → 1.28)
    \item 71.7\% better than BiLSTM+Attention (RMSE: 4.28 → 1.28)
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/results/Stock_Price_Prediction_Forecasting(Next_Day's_Price)_img_9.png}
    \caption{Hybrid Model (SARIMA + XGBoost): Predicted vs Actual Prices}
    \label{fig:hybrid_results}
\end{figure}

\textbf{Visual Analysis (Figure \ref{fig:hybrid_results}):}

The prediction plot demonstrates:
\begin{itemize}
    \item \textbf{Exceptional fit}: Predicted line closely tracks actual prices
    \item \textbf{Trend capture}: Successfully follows the upward trend
    \item \textbf{Volatility handling}: Captures short-term fluctuations better than other models
    \item \textbf{Minimal lag}: Predictions responsive to price changes with minimal delay
    \item \textbf{Consistent performance}: Quality maintained across entire test period
\end{itemize}

\subsubsection{Error Decomposition Analysis}

\textbf{Component Contributions:}

To understand how each component contributes, we analyzed their individual performances:

\begin{table}[h]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Component} & \textbf{RMSE} & \textbf{$R^2$} & \textbf{Role} \\ \midrule
SARIMA Baseline & 3.24 & 0.9891 & Captures trend + seasonality \\
XGBoost on Residuals & 2.51* & N/A & Learns non-linear corrections \\
Hybrid Combined & \textbf{1.28} & \textbf{0.9983} & Synergistic improvement \\ \bottomrule
\end{tabular}
\caption{Hybrid Model Component Analysis (*RMSE of residual predictions)}
\end{table}

\textbf{Why the Hybrid Outperforms:}

\begin{enumerate}
    \item \textbf{Complementary error patterns}:
    \begin{itemize}
        \item SARIMA errors: Systematic deviations from non-linear dynamics
        \item XGBoost corrections: Learned from features SARIMA doesn't use
        \item Combined: Errors partially cancel out
    \end{itemize}
    
    \item \textbf{Stationarity for XGBoost}:
    \begin{itemize}
        \item Residuals approximately stationary (mean ~0, stable variance)
        \item Eliminates extrapolation problem that plagued standalone XGBoost
        \item XGBoost operates in favorable regime
    \end{itemize}
    
    \item \textbf{Statistical rigor + ML flexibility}:
    \begin{itemize}
        \item SARIMA provides theoretically sound baseline
        \item XGBoost adds data-driven adaptability
        \item Best of both paradigms
    \end{itemize}
    
    \item \textbf{Feature diversity}:
    \begin{itemize}
        \item SARIMA uses only past prices
        \item XGBoost leverages technical indicators, volume, temporal features
        \item Richer information set
    \end{itemize}
\end{enumerate}

\textbf{Residual Analysis:}

Examination of hybrid model residuals shows:
\begin{itemize}
    \item \textbf{Near-zero mean}: Residuals centered at 0 (unbiased)
    \item \textbf{Approximately normal}: Distribution close to Gaussian
    \item \textbf{No autocorrelation}: Ljung-Box test confirms white noise residuals
    \item \textbf{Homoskedastic}: Relatively constant variance over time
\end{itemize}

These properties confirm the model has successfully captured both linear and non-linear components, leaving only irreducible noise.

\subsubsection{Practical Interpretation}

\textbf{Real-World Performance:}

For AAPL stock trading:
\begin{itemize}
    \item \textbf{RMSE of 1.28}: On average, predictions off by \$1.28
    \item \textbf{Context}: For a stock trading at ~\$180-220 range, this is ~0.5-0.7\% error
    \item \textbf{Directional accuracy of 87\%}: Critical for trading—model correctly predicts up/down movement 87\% of the time
\end{itemize}

\textbf{Trading Strategy Implications:}

If using this model for a simple trading strategy:
\begin{itemize}
    \item Buy when model predicts price increase
    \item Sell when model predicts price decrease
    \item 87\% directional accuracy suggests profitable potential
    \item Must account for transaction costs, slippage, market impact
\end{itemize}

Note: This is simplified—actual trading requires risk management, position sizing, and consideration of other factors.

\textbf{Computational Efficiency:}

\begin{itemize}
    \item \textbf{Training time}: ~5 minutes total (2 min SARIMA + 3 min XGBoost)
    \item \textbf{Inference time}: \textless 1 second per prediction
    \item \textbf{Faster than deep learning}: 3-4x faster training than LSTM models
    \item \textbf{Scalability}: Can easily run on standard hardware (no GPU required)
\end{itemize}

\subsection{Comprehensive Model Comparison}

\subsubsection{Quantitative Performance Summary}

Table \ref{tab:model_comparison_extended} presents a comprehensive comparison of all implemented models across multiple evaluation metrics.

\begin{table}[htbp]
\centering
\caption{Extended Performance Comparison of All Models}
\label{tab:model_comparison_extended}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Model} & \textbf{RMSE} & \textbf{$R^2$} & \textbf{MAE} & \textbf{Dir. Acc.} & \textbf{Train Time} \\ \midrule
ARIMA & 32.52 & -0.0868 & 28.5 & ~52\% & 1-2 min \\
SARIMA & 3.24 & 0.9891 & 2.5 & ~78\% & 2-3 min \\
Linear Regression & 2.92 & 0.9913 & 2.1 & ~80\% & \textless 1 min \\
Random Forest & 27.79 & 0.2064 & 24.3 & ~55\% & 3-4 min \\
XGBoost & 27.13 & 0.2435 & 23.8 & ~57\% & 3-5 min \\
Univariate LSTM & 4.52 & 0.9792 & 3.8 & ~75\% & 10-15 min \\
BiLSTM + Attention & 4.28 & 0.9722 & 3.5 & ~77\% & 15-20 min \\
\textbf{Hybrid (SARIMA + XGB)} & \textbf{1.28} & \textbf{0.9983} & \textbf{0.95} & \textbf{~87\%} & \textbf{~5 min} \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/results/Stock_Price_Prediction_Forecasting(Next_Day's_Price)_img_10.png}
    \caption{Comparison of Predicted Prices by All Models}
    \label{fig:model_comparison_all}
\end{figure}

\textbf{Visual Comparison Insights (Figure \ref{fig:model_comparison_all}):}

The comprehensive comparison plot reveals:
\begin{itemize}
    \item \textbf{Hybrid model (green)}: Tracks actual prices most closely
    \item \textbf{Linear Regression (orange)}: Good performance, slight systematic deviations
    \item \textbf{SARIMA (red)}: Solid baseline, misses some non-linear patterns
    \item \textbf{LSTM models (purple/pink)}: Good but with occasional lag
    \item \textbf{Tree models (light colors)}: Systematic underprediction—flat-lining effect
    \item \textbf{ARIMA (dashed)}: Completely fails to track—worst performance
\end{itemize}

\subsubsection{Model Category Analysis}

\textbf{Statistical Models:}
\begin{itemize}
    \item \textbf{Best}: SARIMA (RMSE: 3.24, $R^2$: 0.9891)
    \item \textbf{Worst}: ARIMA (RMSE: 32.52, $R^2$: -0.0868)
    \item \textbf{Key lesson}: Seasonal awareness crucial for financial time series
    \item \textbf{Limitation}: Cannot capture non-linear dynamics
\end{itemize}

\textbf{Traditional Machine Learning:}
\begin{itemize}
    \item \textbf{Best}: Linear Regression (RMSE: 2.92, $R^2$: 0.9913)
    \item \textbf{Worst}: Random Forest (RMSE: 27.79, $R^2$: 0.2064)
    \item \textbf{Surprising finding}: Simple linear model outperforms complex ensembles
    \item \textbf{Critical insight}: Feature engineering > model complexity
    \item \textbf{Tree failure}: Fundamental incompatibility with extrapolation
\end{itemize}

\textbf{Deep Learning:}
\begin{itemize}
    \item \textbf{Best}: BiLSTM + Attention (RMSE: 4.28, $R^2$: 0.9722)
    \item \textbf{Improvement}: 5.3\% better than univariate LSTM
    \item \textbf{Trade-off}: Marginal gains at 3.4x parameter cost
    \item \textbf{Conclusion}: Deep learning strong but not optimal for this task
\end{itemize}

\textbf{Hybrid Approach:}
\begin{itemize}
    \item \textbf{Champion}: Hybrid SARIMA+XGBoost (RMSE: 1.28, $R^2$: 0.9983)
    \item \textbf{Dominance}: Outperforms all alternatives by wide margin
    \item \textbf{Efficiency}: Better results in less time than deep learning
    \item \textbf{Interpretability}: More interpretable than black-box LSTM
\end{itemize}

\subsubsection{Performance Tier Classification}

Based on RMSE performance, models fall into distinct tiers:

\textbf{Tier 1 - Excellent (RMSE \textless 2.0):}
\begin{itemize}
    \item Hybrid SARIMA+XGBoost: 1.28
\end{itemize}
Suitable for production deployment and real trading applications.

\textbf{Tier 2 - Good (RMSE 2.0-5.0):}
\begin{itemize}
    \item Linear Regression: 2.92
    \item SARIMA: 3.24
    \item BiLSTM + Attention: 4.28
    \item Univariate LSTM: 4.52
\end{itemize}
Decent performance; suitable for research or non-critical applications.

\textbf{Tier 3 - Poor (RMSE \textgreater 25.0):}
\begin{itemize}
    \item XGBoost: 27.13
    \item Random Forest: 27.79
    \item ARIMA: 32.52
\end{itemize}
Unsuitable for stock price prediction without significant modification.
